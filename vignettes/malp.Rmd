---
title: "The Maximum Agreement Prediction via the Concordance Correlation Coefficient"
author: "Taeho Kim^[Lehigh University, tak422@lehigh.edu], George Luta^[Georgetown University, George.Luta@georgetown.edu], Matteo Bottai^[Karolinska Institutet, matteo.bottai@ki.se], Pierre Chausse^[University of Waterloo, pchausse@uwaterloo.ca], Gheorghe Doros^[Boston University,, doros@bu.edu], Edsel A. Pena^[University of South Carolina, pena@stat.sc.edu]"
date: ""
output: rmarkdown::html_vignette
abstract: "The vignette explains how to use the malp package to compute maximum agreement prection, construct confidence intervals for the prediction and illustrate the result."
vignette: >
  %\VignetteIndexEntry{The Maximum Agreement Prediction via the Concordance Correlation Coefficient}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
header-includes:
- \newcommand{\trp}{^{\tt T}}
- \DeclareMathOperator{\tr}{tr}
- \newcommand{\trace}{\mbox{\rm\bf tr}}
- \DeclareMathOperator*{\argmax}{arg\,max}
- \DeclareMathOperator*{\argmin}{arg\,min}
- \newcommand{\E}{\mathrm{E}}
- \newcommand{\Var}{\mathrm{Var}}
- \newcommand{\Cov}{\mathrm{Cov}}
- \newcommand{\Cor}{\mathrm{Cor}}
- \newcommand{\CCC}{\mathrm{CCC}}
- \newcommand{\PCC}{\mathrm{PCC}}
- \newcommand{\MSE}{\mathrm{MSE}}
- \newcommand{\bs}{\boldsymbol} 
---


# Introduction

Suppose we have a $p\times 1$ vector of covariates $x$ and a dependent
variable $Y$. The MALP predictor is defined as:

\[
\tilde{Y}^\star(x)   = 
  \left(1-1/\gamma\right)\mu_\mathrm{Y}+ \left( 1/\gamma\right) \tilde{Y}^\dagger(x)\,,
\]

where $\gamma$ is the concordance correlation coefficient (CCC),
$\mu_\mathrm{Y}$ is the population mean of Y and
$\tilde{Y}^\dagger(x)$ is the best linear predictor. For any predictor
$\tilde{Y}$, the CCC is defined as

\[
\gamma = \frac{2\sigma_{\mathrm{Y}\tilde{\mathrm{Y}}}}{
\sigma^2_\mathrm{Y}+\sigma^2_{\tilde{\mathrm{Y}}}+(\mu_\mathrm{Y}-\mu_{\tilde{\mathrm{Y}}})}\,,
\]

where $\sigma_{xy}$ is the covariance between $x$ and $y$ and
$\sigma^2_x$ is the variance of $x$. When the predictor is the MALP
defined above, the CCC is equal to the square root of the coefficient
of determination $R^2$ of the best linear predictor.

Let $X=\{1,x'\}'$ and let the best linear predictor
$\tilde{Y}^\dagger(x)$ be $X'\beta\equiv \beta_1+x'\beta_2$, where
$\beta_1=\mu_\mathrm{Y}-\mu_\mathrm{x}'\beta_2$ and
$\beta_2=\Var(X)^{-1}\Cov(X,Y)$, then the MALP can be written as:

\begin{eqnarray*}
\tilde{Y}^\star(x)  & = & 
  \left(1-1/\gamma\right)\mu_\mathrm{Y}+ \left( 1/\gamma\right) \left[X'\beta\right]\\
&=& \left[(1-1/\gamma)\mu_\mathrm{Y}+\beta_1/\gamma\right] + 
x'[\beta_2/\gamma] \\
&\equiv & \alpha_1 + x'\alpha_2\\
&\equiv & X'\alpha
\end{eqnarray*}

Assuming we have an IID sample $\{Y_i,x_i\}$ of size $n$, a consistent estimator of the MALP at $x=x_0$ is:

\[
\hat{Y}^\star(x_0) = \hat{\alpha}_1 + x_0'\hat{\alpha}_2\,
\]

where
$\hat{\alpha}_1=(1-1/\hat\gamma)\overline{\mathrm{Y}}+\hat\beta_1/\hat\gamma$,
$\hat{\alpha}_2 = \hat{\beta}_2/\hat\gamma$, $\hat\beta_1$ and
$\hat{\beta}_2$ are the least square estimators,
$\overline{\mathrm{Y}}$ is the sample mean of Y and $\hat\gamma$ is
the square root of the least square coefficient of determination.

If we assume that $\{Y,x\}$ are jointly normal, the MALP predictor
$\hat{Y}^\star(x_0)$ is asymptotically normal with the following
variance:

\begin{eqnarray*}
\sigma_\mathrm{MA}^2(x_0)  &=&  \sigma_\mathrm{Y}^2(1-\gamma^2) \times  \\
 & & \left[\frac{2}{1 + \gamma} +  \frac{1}{\gamma^2} (x_0 - \mu_\mathrm{X})' \Var(\mathrm{X})^{-1} (x_0 - \mu_\mathrm{X}) 
- \frac{(1-\gamma^2)}{\sigma_\mathrm{Y}^2 \gamma^4} \left[\Cov(\mathrm{X},\mathrm{Y})'\Var(\mathrm{X})^{-1} (x_0 - \mu_\mathrm{X})\right]^2\right]\\
&=&  \sigma_\mathrm{Y}^2(1-\gamma^2) \times  \\
 & & \left[\frac{2}{1 + \gamma} +  \frac{1}{\gamma^2} (x_0 - \mu_\mathrm{X})' \Var(\mathrm{X})^{-1} (x_0 - \mu_\mathrm{X}) 
- \frac{(1-\gamma^2)}{\sigma_\mathrm{Y}^2 \gamma^4} \left[\tilde{Y}^\dagger(x_0)-\mu_\mathrm{Y}\right]^2\right]\\
\end{eqnarray*}

Since the asymptotic variance of the estimated best linear predictor
is

\[
\sigma_\mathrm{LS}^2(x_0) = \sigma_\mathrm{Y}^2(1-\gamma^2) \left[1 + (x_0 - \mu_\mathrm{X}) \Sigma_\mathrm{XX}^{-1}(x_0 - \mu_\mathrm{X})'\right]
\]

We can write $\sigma_\mathrm{MA}^2(x_0)$ as a function of $\sigma_\mathrm{LS}^2(x_0)$:

\[
\sigma_\mathrm{MA}^2(x_0) = 
\frac{\sigma_\mathrm{LS}^2(x_0)}{\gamma^2} + 
\frac{\sigma_\mathrm{Y}^2(1-\gamma^2)}{\gamma^2} \times
\left[
\frac{2\gamma^2-\gamma-1}{1+\gamma} -
\frac{(1-\gamma^2)}{\sigma_\mathrm{Y}^2 \gamma^2}[\tilde{Y}^\dagger(x_0)-\mu_\mathrm{Y}]^2
\right]
\]

We obtain consistent estimator of the variances by replacing the
population values of $\gamma$, $\sigma_\mathrm{LS}^2(x_0)$,
$\sigma_\mathrm{Y}^2$, $\mu_\mathrm{X}$, $\Sigma_{\mathrm{XX}}$,
$\tilde{Y}^\dagger(x_0)$ and $\mu_\mathrm{Y}$ by their sample
estimates. Note that there is no unique way to estimate
$\sigma_\mathrm{LS}^2(x_0)$. For example, if we estimate it using the following:

\begin{eqnarray*}
\hat\sigma_\mathrm{Y}^2 &=& \frac{1}{n-1}\sum_{i=1}^n (Y_i-\bar{Y})^2\\
\hat\gamma &=& \frac{\sum_{i=1}^n (Y_i-\bar{Y})(\hat{Y}^\star_i-\bar{Y})}{
\sqrt{\sum_{i=1}^n (Y_i-\bar{Y})^2\sum_{i=1}^n (\hat{Y}^\star_i-\bar{Y})^2}}\\
\hat\Sigma_\mathrm{XX}&=& \frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})(X_i-\bar{X})'\,,
\end{eqnarray*}

which is the default method that we use in the package, we have the
following relationship between this estimator and the one computed by
the `predict` method for `lm` objects. 

\[
\hat\sigma_\mathrm{LS}^{2}(x_0) = \frac{n-p-1}{n}\hat\sigma_\mathrm{LS}^{2(R)}(x_0) + \frac{\hat\sigma_\mathrm{Y}^2(1-\hat{\gamma}^2)}{n}\,,
\]

where $\hat\sigma_\mathrm{LS}^{2(R)}$ is the one computed by
`predict.lm` and $\hat\sigma_\mathrm{LS}^{2}(x_0)$ is the default
estimator computed in the `malp` package. The difference comes for how
the different estimators from the expression are adjusted for the loss
of degrees of freedom. The package offer the option of using the
`predict.lm` correction.

# The `malp` package

The main function is `malp`, which returns an object of class
`malp`. The purpose of this function is to compute the estimates
$\hat\alpha$. The function has two arguments: `formula` and
`data`. The former is like the formula provided to `lm` for linear
regressions and `data` is a `data.frame` containing all variables
included in the formula. In the following example, we have one
independent variable and one dependent variable.

```{r}
## Data just for the testing
library(malp)
set.seed(11223344)
x<-rnorm(100)
y<-1+2*x+rnorm(100)
dat <- data.frame(x,y)
fit <- malp(y~x, dat)
```

The `malp` object has its own `print` method that returns the
coefficient estimates $\hat\alpha$.

```{r}
print(fit, digits=5)
```

## The `vcov` method

Currently, there is no closed form expression for the variance of the
coefficients. The only possible option for now is through simulation
methods. There are two options for the simulation method: Bootstrap or
Jackknife. For the bootstrap method, the number of bootstrap samples
is set by the argument `B`. For example:

```{r}
v1 <- vcov(fit, "Boot", B=100)
v2 <- vcov(fit, "Jackknife")
```

## The `summary` method

This method returns detailed information about the estimation. In
particular, it returns the standard errors, t-ratios and p-values of
the $\hat{\alpha}$. The arguments of the function are to specify how
to compute the standard errors. The options are the same as for
`vcov`. The method returns an object of class `summary.malp`, which
has its own `print` method.

```{r}
print(summary(fit, "Boot", B=100), digits=5)
```

If we only want to see the value of the MSE, PCC and CCC, we can avoid
computing the standard errors, which can be a problem for very large
samples, by setting the argument `se` to `FALSE`:

```{r}
(s <- summary(fit, se=FALSE))
```

These "good fit" measures can also be exctracted from the `summary`
output. It includes the measure for both MALP and LSLP

```{r}
cbind(MALP=s$fitMALP, LSLP=s$fitLSLP)
```

## The `predict` method

The method works like the `predict.lm` method. By default, it predicts
the dependent variables for the same values of the covariates used to
fit the model.

```{r}
pr <- predict(fit)
pr[1:4]
```

In that case, it returns predicted values only. If the argument
`se.fit` is set to `TRUE`, it returns a list with the element `fit`
being the predicted values and the argument `se.fit` being the
standard errors.

```{r}
pr <- predict(fit, se.fit=TRUE)
pr$fit[1:4]
pr$se.fit[1:4]
```

By default, the standard errors returned by `predict` are based on the
asymptotic theory under the assumption of the joint normality of
$\{Y,x\}$. It uses the expression from the Introduction section for
$\hat\sigma_\mathrm{MA}^2(x_0)$ with
$\hat\sigma_\mathrm{LS}^{2}(x_0)$. For standard errors based on the
`predict.lm` version $\hat\sigma_\mathrm{LS}^{2(R)}(x_0)$, we set the
argument `LSdfCorr` to `TRUE` (it stands for Least Squares degrees of
freedom Correction).

```{r}
pr <- predict(fit, se.fit=TRUE, LSdfCorr=TRUE)
pr$se.fit[1:4]
```

If we are not willing to assume normality, we have the option of
computing the standard errors using the expression
$\sqrt{X_0'\Var(\hat\alpha)X_0}$, where $X_0=\{1,x_0'\}'$ and
$\Var(\hat\alpha)$ is computed with `vcov`. All we need to do is to
set the argument `vcovMet` to either "Boot" or "Jackknife". For the
"Boot" option, the number of bootstrap sample is set by the argument
`Bse.`

```{r}
pr <- predict(fit, se.fit=TRUE, vcovMet="Boot", Bse.=100)
pr$se.fit[1:4]
```

If we want to predict $Y$ for specific values of $X$, we can pass the
specific values to the argument `newdata` as a `data.frame`. The
data.frame must contain values for all covariates in the formula.

```{r}
newd <- data.frame(x=c(-.3,.3,1.5))
predict(fit, newdata=newd)
```

Note that the CCC can be computed manually using the `ccc` function
included in the package. We first use predict to get the fitted values
and then compute the CCC:

```{r}
yhat <- predict(fit)
ccc(y,yhat)
```

## Confidence Intervals

The confidence intervals for the predictor also comes from the
`predict` method, but given the different options, it deserves its own
section. By default, parametric confidence intervals are produced by
setting the argument `interval` to "confidence".

```{r}
pr <- predict(fit, newdata=newd, interval="confidence")
pr
```

The options for standard errors used to compute the parametric
confidence intervals are explained in the previous section. For
example, we can construct the intervals using the Jackknife standard
errors this way:

```{r}
pr <- predict(fit, newdata=newd, interval="confidence", vcovMet="Jackknife")
pr
```

Note that if the argument `se.fit` is set to `TRUE`, it returns a list
with the element `fit` being the intervals and the element `se.fit`
being the standard errors.

It is also possible to compute bootstrap confidence intervals. The options are 

- **norm**: Normal interval

- **basic**: Basic interval

- **stud**: Studentized intervals

- **perc**: Percentile intervals

- **bca**: Bias corrected intervals.

To obtain one of these bootstrap confidence intervals, we set the
argument `bootInterval` to `TRUE`. The type of interval is obtained by
setting the argument `bootIntType` to one of the above options. If set
to "all", the default, the functions returns all interval in a
list. These intervals are computed using `boot` and `boot.ci` from the
`boot` package. For the studentized interval, `se.fit` must be set to
`TRUE`. If `bootIntType` it set to "all" and `se.fit` to `FALSE`, the
function will not return a studentized interval. 

With `bootInterval=TRUE`, the function return a list of intervals. The
name of each element is the interval type. If `bootIntType` is not set
to "all", the function return a list of length equal to 1. 

```{r}
pr <- predict(fit, newdata=newd, bootInterval=TRUE, se.fit=TRUE,
              vcovMet="Jackknife", B.=100)
pr$norm
pr$stud
pr$bca
pr$perc
```

It is possible to speedup the process by changing the arguments
`parallel`, `ncpus` and `cl`. See the help file of the `boot` function
for more details. For example, the following would compute the
bootstrap interval in parallel using 8 CPU's

```{r, eval=FALSE}
pr <- predict(fit, newdata=newd, bootInterval=TRUE, se.fit=TRUE,
              vcovMet="Jackknife", B.=100, parallel="multicore",
              ncpus=8)
```

## Prediction intervals

The prediction intervals for LSLP and MALP are respectively:

\[
\left[\hat{Y}^\dagger(x_0) \pm z_{\alpha/2} \sqrt{S_\mathrm{Y}(1-\gamma^2)+
\sigma^2_\mathrm{LS}(x_0)/n}\right]
\]

and

\[
\left[\hat{Y}^\star(x_0) + \hat{b}(x_0) \pm z_{\alpha/2} \sqrt{S_\mathrm{Y}(1-\gamma^2)+
\sigma^2_\mathrm{MA}(x_0)/n}\right]
\]

where $\hat{b}(x_0)$ is the prediction bias of MALP. It turns out that
$\hat{Y}^\star(x_0) + \hat{b}(x_0)=\hat{Y}^\dagger(x_0)$, so the
prediction interval for MALP can be written as

\[
\left[\hat{Y}^\dagger(x_0) \pm z_{\alpha/2} \sqrt{S_\mathrm{Y}(1-\gamma^2)+
\sigma^2_\mathrm{MA}(x_0)/n}\right]
\]

Since $S_\mathrm{Y}(1-\gamma^2)$ is the estimated variance of the
error term of least squares models, if the argument `LSdfCorr` is set
to `TRUE`, $S_\mathrm{Y}(1-\gamma^2)$ is multiplied $(n-1)/df$, where
$df$ is the least squares residuals degrees of freedom. Note that
bootstrap intervals are not available for prediction intervals. For
valid intervals in case of non-normality, we can set the argument
`vcovMet` to either "Boot" or "Jackknife" to obtain a consistent
estimator of $\sigma^2_\mathrm{MA}(x_0)$. 

```{r}
pr1 <- predict(fit, newdata=newd, interval="prediction",
              vcovMet="Jackknife", includeLS=TRUE)
pr2 <- predict(fit, newdata=newd, interval="confidence",
               vcovMet="Jackknife")
pr1$MALP
pr2
```

Since only the variance differs between the prediction interval of
LSLP amnd MALP, they are very close to each other:

```{r}
pr1$LSLP
```

## The `plot` method

This method produces a scatter plot of the dependent variable against
the fitted values from the MALP, LSLP or both. A 45 degree line is
added to better evaluate the level of agreement. The following shows
one example with `which="MALP"`. The other options are "LSLP" and
"Both".

```{r, fig.align='center'}
plot(fit, which="MALP", bg=NA, col=1)
```
