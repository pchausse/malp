---
title: "The Maximum Agreement Prediction via the Concordance Correlation Coefficient"
author: "Taeho Kim^[Lehigh University, tak422@lehigh.edu], George Luta^[Georgetown University, George.Luta@georgetown.edu], Matteo Bottai^[Karolinska Institutet, matteo.bottai@ki.se], Pierre Chausse^[University of Waterloo, pchausse@uwaterloo.ca], Gheorghe Doros^[Boston University,, doros@bu.edu], Edsel A. Pena^[University of South Carolina, pena@stat.sc.edu]"
date: ""
output:
 pdf_document:
   number_sections: true

abstract: "The vignette explains how to use the malp package to compute maximum agreement prection, construct confidence intervals for the prediction and illustrate the result."
vignette: >
  %\VignetteIndexEntry{The Maximum Agreement Prediction via the Concordance Correlation Coefficient}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
header-includes:
- \newcommand{\trp}{^{\tt T}}
- \DeclareMathOperator{\tr}{tr}
- \newcommand{\trace}{\mbox{\rm\bf tr}}
- \DeclareMathOperator*{\argmax}{arg\,max}
- \DeclareMathOperator*{\argmin}{arg\,min}
- \newcommand{\E}{\mathrm{E}}
- \newcommand{\Var}{\mathrm{Var}}
- \newcommand{\Cov}{\mathrm{Cov}}
- \newcommand{\Cor}{\mathrm{Cor}}
- \newcommand{\CCC}{\mathrm{CCC}}
- \newcommand{\PCC}{\mathrm{PCC}}
- \newcommand{\MSE}{\mathrm{MSE}}
- \newcommand{\bs}{\boldsymbol} 
---

```{r, include=FALSE}
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "footnotesize","\n\n", x, "\n\n \\normalsize")
})
knitr::opts_chunk$set(comment="")
```

# Introduction

Suppose we have a $p\times 1$ vector of covariates $x_0$ and a dependent
variable $Y$. The MALP predictor is defined as:

\begin{eqnarray*}
\tilde{Y}^\star(x_0)   &=& 
\mu_Y + \frac{1}{\gamma}\Sigma_{XY}'\Sigma_X^{-1}(x_0-\mu_X)\\
&=& \left(1-1/\gamma\right)\mu_Y+ \left( 1/\gamma\right) \tilde{Y}^\dagger(x_0)\,,
\end{eqnarray*}

where $\gamma$ is the concordance correlation coefficient (CCC),
$\mu_Y$ is the population mean of Y ,$\Sigma_X$ is the covariance
matrix of the random vector $X$, $\Sigma_{XY}$ is the vector of covariance
between $X$ and $Y$ and $\tilde{Y}^\dagger(x_0)$ is the best linear
predictor defined as

\begin{eqnarray*}
\tilde{Y}^\dagger(x_0) &=& \beta_1 + \beta_2'x_0\\
&=& (\mu_Y -\Sigma_{XY}'\Sigma_X^{-1}\mu_X) + \Sigma_{XY}'\Sigma^{-1}_Xx_0\\
&=& \mu_Y + \Sigma_{XY}'\Sigma^{-1}_X (x_0-\mu_X)
\end{eqnarray*}

The CCC $\gamma$ is defined as 

\begin{eqnarray*}
\gamma &=& \frac{[\Sigma_{XY}'\Sigma_X^{-1}\Sigma_{XY}]^{1/2}}{\sigma_Y}\\
&=& \frac{[\beta_2'\Sigma_X\beta_2]^{1/2}}{\sigma_Y}\,.
\end{eqnarray*}

It is therefore the square root of the coefficient of determination
$R^2$ from the regression of $Y$ oin $X$. Let $X_0$ be the
$(p+1)\times 1$ vector $\{1,~x_0'\}'$. Then the MALP predictor can
also be written as:

\begin{eqnarray*}
\tilde{Y}^\star(x)  & = & 
  \left(1-1/\gamma\right)\mu_\mathrm{Y}+ (\beta_1+x_0'\beta_2)/\gamma\\
&=& \left[(1-1/\gamma)\mu_\mathrm{Y}+\beta_1/\gamma\right] + 
x_0'[\beta_2/\gamma] \\
&\equiv & \alpha_1 + x_0'\alpha_2\\
&\equiv & X'\alpha
\end{eqnarray*}

Assuming we have an IID sample $\{Y_i,x_i\}$ of size $n$, a consistent estimator of the MALP at $x=x_0$ is:

\[
\hat{Y}^\star(x_0) = \hat{\alpha}_1 + x_0'\hat{\alpha}_2\,
\]

where
$\hat{\alpha}_1=(1-1/\hat\gamma)\overline{\mathrm{Y}}+\hat\beta_1/\hat\gamma$,
$\hat{\alpha}_2 = \hat{\beta}_2/\hat\gamma$, $\hat\beta_1$ and
$\hat{\beta}_2$ are the least square estimators,
$\overline{\mathrm{Y}}$ is the sample mean of Y and $\hat\gamma$ is
the square root of the least square coefficient of determination.

To obtain the variance of the MALP predictor under general
distribution, we first note that $\tilde{Y}^\star(x_0)$ can be written
as a non-linear function $g(\theta;x_0)$, where

\[
\theta =
\begin{pmatrix}
\mu_Y\\
\mu_X\\
\sigma^2_Y\\
\Sigma_{XY}\\
\mathrm{Vec}(\Sigma_X)
\end{pmatrix}
\]

If we define the ith observation $T_i$ from an iid sample as

\[
T_i = \begin{pmatrix}
Y_i\\
X_i\\
(Y_i-\bar{Y})^2\\
(X_i-\bar{X})(Y_i-\bar{Y})\\
\mathrm{Vec}((X_i-\bar{X})(X_i-\bar{X})')
\end{pmatrix}\,.
\]

and we assume that $\text{E}(\|\{Y,X'\}'\|^4)<\infty$, then
$\sqrt{n}(\bar{T}-\theta)\overset{d}{\to} N(0,\Xi)$, by the
Lindeberg-LÃ©vy Central Limit Theorem. Then, using the Delta Method, $\sqrt{n}[\hat{Y}^\star(x_0)-\tilde{Y}^\star(x_0)]\overset{d}{\to} N(0,~\nabla g(\theta;x_0)'\Xi\nabla g(\theta;x_0))$, where

\[
\nabla g(\theta;x_0) = \begin{pmatrix}
1\\
-\frac{\beta_2}{\gamma}\\
\frac{\beta_2'(x_0-\mu_X)}{2\gamma\sigma^2_Y}\\
\frac{\Sigma_X^{-1}(x_0-\mu_X)}{\gamma}-
\frac{[\beta_2'(x_0-\mu_X)]\beta_2}{\gamma^3\sigma^2_Y}\\
-\frac{\text{Vec}[\Sigma_X^{-1}(x_0-\mu_X)\beta_2']}{\gamma}
+ \frac{[\beta_2'(x_0-\mu_X)]\text{Vec}(\beta_2\beta_2')}{2\gamma^3\sigma^2_Y}
\end{pmatrix}
\]

We can estimate this variance using the sample estimates of $\beta_2$,
$\gamma$, $\sigma^2_Y$, $\Sigma_X$, $\mu_X$ and $\Xi$. The
covariance matrix of $\hat\alpha$ can be obtained by using the fact that:

\[
\text{Var}[\hat{Y}^\star(x_0)] = \begin{pmatrix}
1&x_0' \end{pmatrix}\text{Var}(\hat\alpha)
\begin{pmatrix}
1\\x_0 \end{pmatrix}
\]

By rearranging, we get obtain the following result:

\[
\text{Var}\left[\sqrt{n}(\hat{\alpha}-\alpha)\right]
\to \begin{pmatrix}
(\mu_X'A'\Xi_{22} A \mu_X- 2\mu_x'A'\Xi_{21} B + 
\sigma^2_Y(1-\gamma)) & 
(B'\Xi_{12} A- \mu_X' A'\Xi_{22} A)\\
(A'\Xi_{21} B- A'\Xi_{22} A \mu_X) & 
A'\Xi_{22} A
\end{pmatrix}\,,
\]

where

\[
A =
\begin{pmatrix}
\frac{\beta_2'}{2\gamma\sigma_Y^2}\\
\frac{\Sigma_X^{-1}}{\gamma}-\frac{\beta_2\beta_2'}{\gamma^3\sigma^2_Y}\\
\frac{-\beta_2\otimes \Sigma_X^{-1}}{\gamma}+
\frac{Vec(\beta_2\beta_2')\beta_2'}{2\gamma^3\sigma^2_Y}
\end{pmatrix}~~\mbox{,}~~
B =
\begin{pmatrix}
1\\
-\frac{\beta_2}{\gamma}
\end{pmatrix}
\]

and $\Xi_{ij}$ are partitions of $\Xi$.


In general, there is no closed-form expression for the variance, but
if we assume that the vector $\{Y,X'\}'$ is normally distributed, we
can show that the above asymptotic variance can be written as:

\begin{eqnarray*}
\sigma_\mathrm{MA}^2(x_0)  &=&  \sigma_\mathrm{Y}^2(1-\gamma^2)
 \left[\frac{2}{1 + \gamma} +
 \frac{(x_0 - \mu_X)' \Sigma_X^{-1} (x_0 - \mu_X)}{\gamma^2} 
- \frac{(1-\gamma^2)}{\sigma_Y^2 \gamma^4} 
\left[\beta_2'(x_0 - \mu_X)\right]^2\right]
\end{eqnarray*}

Since the asymptotic variance of the estimated best linear predictor
under the homoskedasticity assumption is

\[
\sigma_\mathrm{LS}^2(x_0) = \sigma_Y^2(1-\gamma^2) \left[1 + (x_0 - \mu_X) \Sigma_X^{-1}(x_0 - \mu_X)'\right]
\]

We can write $\sigma_\mathrm{MA}^2(x_0)$ as a function of $\sigma_\mathrm{LS}^2(x_0)$:

\[
\sigma_\mathrm{MA}^2(x_0) = 
\frac{\sigma_\mathrm{LS}^2(x_0)}{\gamma^2} + 
\frac{\sigma_Y^2(1-\gamma^2)}{\gamma^2} \times \left[
\frac{2\gamma^2-\gamma-1}{1+\gamma} -
\frac{(1-\gamma^2)}{\sigma_Y^2 \gamma^2}[\beta_2'(x_0-\mu_X)]^2
\right]
\]

We obtain consistent estimator of the variances by replacing the
population values of $\gamma$, $\sigma_\mathrm{LS}^2(x_0)$,
$\sigma_Y^2$, $\mu_X$, $\Sigma_X$, $\beta_2$ and $\mu_Y$ by
their sample estimates. Note that there is no unique way to estimate
$\sigma_\mathrm{LS}^2(x_0)$. For example, if we estimate it using the
following:

\begin{eqnarray*}
\hat\sigma_Y^2 &=& \frac{1}{n-1}\sum_{i=1}^n (Y_i-\bar{Y})^2\\
\hat\gamma &=& \frac{[\hat{\beta}_2'\hat\Sigma_X\hat\beta_2]^{1/2}}{\hat\sigma_Y}\\
\hat\Sigma_X&=& \frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})(X_i-\bar{X})'\,,
\end{eqnarray*}

which is the default method that we use in the package, we have the
following relationship between this estimator and the one computed by
the `predict` method for `lm` objects. 

\[
\hat\sigma_\mathrm{LS}^{2}(x_0) = \frac{n-p-1}{n}\hat\sigma_\mathrm{LS}^{2(\text{R})}(x_0) + \frac{\hat\sigma_Y^2(1-\hat{\gamma}^2)}{n}\,,
\]

where $\hat\sigma_\mathrm{LS}^{2(\text{R})}$ is the one computed by
`predict.lm` and $\hat\sigma_\mathrm{LS}^{2}(x_0)$ is the default
estimator computed in the `malp` package. The difference comes from how
the different estimators from the expression are adjusted for the loss
of degrees of freedom. The package offer the option of using the
`predict.lm` correction.

Using the same approach as for the general case, we can obtain the following
closed-form expression for the asymptotic covariance matrix of $\sqrt{n}(\hat\alpha-\alpha)$ under the normality:

\[
\text{Var}\left[\sqrt{n}(\hat{\alpha}-\alpha)\right]
\to\begin{pmatrix}
\frac{2\sigma^2}{1+\gamma} + 
	\mu_X\left(\frac{\sigma^2}{\gamma^2}\Sigma_X^{-1} - 
	\frac{(1-\gamma^2)^2}{\gamma^4}\beta_2\beta_2'
\right)\mu_X' && 
\mu_X \left(-\frac{\sigma^2}{\gamma^2}\Sigma_X^{-1}+\frac{(1-\gamma^2)^2}{\gamma^4}
\beta_2\beta_2'\right)\\
\left(-\frac{\sigma^2}{\gamma^2}\Sigma_X^{-1}+\frac{(1-\gamma^2)^2}{\gamma^4}
\beta_2\beta_2'\right)\mu_X' &&
\frac{\sigma^2}{\gamma^2}\Sigma_X^{-1} - \frac{(1-\gamma^2)^2}{\gamma^4}\beta_2\beta_2'
\end{pmatrix}\,,
\]

where $\sigma^2 = \sigma^2_Y(1-\gamma^2)$ is the variance of the least
squares residuals. We can simply this expression further by using the
fact that the asymptotic variance $\Omega$ of $\hat\beta_2$ under the
homoskedasticity assumption, which is implied by the normality
assumption, is $\sigma^2\Sigma_X^{-1}$ and
$\mu_X'\beta_2=(\mu_Y-\beta_1)$:

\[
\text{Var}\left[\sqrt{n}(\hat{\alpha}-\alpha)\right]
\to\begin{pmatrix}
\frac{2\sigma^2}{1+\gamma} + 
	\frac{1}{\gamma^2} \mu_\mathrm{X}\Omega\mu_\mathrm{X}' - 
	\frac{(1-\gamma^2)^2}{\gamma^4}(\mu_\mathrm{Y}-\beta_1)^2 && 
-\frac{1}{\gamma^2}\mu_\mathrm{X}\Omega+\frac{(1-\gamma^2)^2}{\gamma^4}
(\mu_\mathrm{Y}-\beta_1)\beta_2'\\
-\frac{1}{\gamma^2}\Omega\mu_\mathrm{X}'+\frac{(1-\gamma^2)^2}{\gamma^4}
(\mu_\mathrm{Y}-\beta_1)\beta_2 &&
\frac{1}{\gamma^2}\Omega - \frac{(1-\gamma^2)^2}{\gamma^4}\beta_2\beta_2'
\end{pmatrix}\,,
\]


# The `malp` package

The main function is `malp`, which returns an object of class
`malp`. The purpose of this function is to compute the estimates
$\hat\alpha$. The function has two arguments: `formula` and
`data`. The former is like the formula provided to `lm` for linear
regressions and `data` is a `data.frame` containing all variables
included in the formula. In the following example, we have one
independent variable and one dependent variable.

```{r}
## Data just for the testing
library(malp)
set.seed(11223344)
x<-rnorm(100)
y<-1+2*x+rnorm(100)
dat <- data.frame(x,y)
fit <- malp(y~x, dat)
```

The `malp` object has its own `print` method that returns the
coefficient estimates $\hat\alpha$.

```{r}
print(fit, digits=5)
```

The object is a list with the following elements:

- `coefficients`: The vector $\hat\alpha$
- `gamma`: The value of $\hat\gamma$
- `lm`: The `lm` object from the least squares regression
- `varY`: The value of $\hat\sigma^2_Y$
- `na.action`: The observations that were removed due to missing values. 
- `data`: The `data.frame` used for the estimation. If missing values
	were present, it excludes them.


## The `vcov` method

By default, the covariance matrix of the MALP coefficients is computed
without assuming normality. This is the best option, because it is
also valid under normality. The method returns a $(p+1)\times (p+1)$
covariance matrix named by the variable names in the formula:

```{r}
vcov(fit)
```

To compute the variance under the normality assumption using the
closed-form expression presented in the previous section, we set the
argument `method` to `"Normal"`:

```{r}
vcov(fit, method="Normal")
```

Since the covariance matrices derived in the previous section are
based on the Delta method, we may want to rely on simulation
techniques if the sample size is too small.  The package offers two
options: Bootstrap or Jackknife. For the bootstrap method, the number
of bootstrap samples is set by the argument `B`. For example, the
first method below computes the estimate using 100 bootstrap samples
and the second computes the jackknife estimate:

```{r}
v1 <- vcov(fit, method="Boot", B=100)
v1
v2 <- vcov(fit, method="Jackknife")
```

By default, the covariance matrix under normality is not based on
least squares degrees of freedom correction. If we want this
correction, which only applies under normality, we set the argument
`LSdfCorr` to `TRUE`.

```{r}
vcov(fit, method="Normal", LSdfCorr=TRUE)
```

## The `summary` method

This method returns detailed information about the estimation. In
particular, it returns the standard errors, t-ratios and p-values of
the $\hat{\alpha}$. The arguments of the function are used to specify
how to compute the standard errors. The options are the same as for
`vcov`, but the argument that specifies the method is `vcovMet`. By
default, the standard errors are based on the general expression for
unknown distributions. The method returns an object of class
`summary.malp`, which has its own `print` method.

```{r}
print(summary(fit), digits=5)
print(summary(fit, "Boot", B=100), digits=5)
```

The three different good fit measures printed by `summary` can also be
extracted manually for both the MALP and LS fits:

```{r}
s <- summary(fit)
cbind(MALP=s$fitMALP, LSLP=s$fitLSLP)
```

## The `predict` method

The method works like the `predict.lm` method. By default, it predicts
the dependent variables for the same values of the covariates used to
fit the model. The following prints the first four MALP predictors.

```{r}
pr <- predict(fit)
pr[1:4]
```

If the argument `se.fit` is set to `TRUE`, it returns a list with the
element `fit` being the predicted values and the argument `se.fit`
being the standard errors of the MALP predictions.

```{r}
pr <- predict(fit, se.fit=TRUE)
pr$fit[1:4]
pr$se.fit[1:4]
```

As for `vcov` and `summary`, the default standard errors returned by `predict`
are based on the asymptotic theory for unknown distributions. We can
also use the closed-form expression under normality by setting the
argument `vcovMet` to `"Normal"`. For standard errors based on the
`predict.lm` version $\hat\sigma_\mathrm{LS}^{2(\text{R})}(x_0)$, we
set the argument `LSdfCorr` to `TRUE` (it stands for Least Squares
degrees of freedom Correction).

```{r}
pr <- predict(fit, se.fit=TRUE, vcovMet="Normal", LSdfCorr=TRUE)
pr$se.fit[1:4]
```

For small samples, we also have the option of using simulated
methods. All we need is to set the argument `vcovMet` to either "Boot"
or "Jackknife". For the "Boot" option, the number of bootstrap sample
is set by the argument `Bse.`

```{r}
pr <- predict(fit, se.fit=TRUE, vcovMet="Boot", Bse.=100)
pr$se.fit[1:4]
```

If we want to predict $Y$ for specific values of $X$, we can pass the
specific values to the argument `newdata` as a `data.frame`. The
data.frame must contain values for all covariates in the formula.

```{r}
newd <- data.frame(x=c(-.3,.3,1.5))
predict(fit, newdata=newd)
```

Note that the CCC can be computed manually using the `ccc` function
included in the package. We first use predict to get the fitted values
and then compute the CCC:

```{r}
yhat <- predict(fit)
ccc(y,yhat)
```

## Confidence Intervals

The confidence intervals for the predictor also comes from the
`predict` method, but given the different options, it deserves its own
section. By default, parametric confidence intervals are produced by
setting the argument `interval` to "confidence".

```{r}
pr <- predict(fit, newdata=newd, interval="confidence")
pr
```

The options for standard errors used to compute the parametric
confidence intervals are explained in the previous section. For
example, we can construct the intervals using the Jackknife standard
errors this way:

```{r}
pr <- predict(fit, newdata=newd, interval="confidence", vcovMet="Jackknife")
pr
```

Note that if the argument `se.fit` is set to `TRUE`, it returns a list
with the element `fit` being the intervals and the element `se.fit`
being the standard errors. 

It is also possible to compute bootstrap confidence intervals. The options are 

- **norm**: Normal interval

- **basic**: Basic interval

- **stud**: Studentized intervals

- **perc**: Percentile intervals

- **bca**: Bias corrected intervals.

To obtain one of these bootstrap confidence intervals, we set the
argument `bootInterval` to `TRUE`. The type of interval is obtained by
setting the argument `bootIntType` to one of the above options. If set
to "all", the default, the functions returns all interval in a
list. These intervals are computed using `boot` and `boot.ci` from the
`boot` package. For the studentized interval, `se.fit` must be set to
`TRUE`. If `bootIntType` it set to "all" and `se.fit` to `FALSE`, the
function will not return a studentized interval. 

With `bootInterval=TRUE`, the function returns a list of intervals. The
name of each element is the interval type. If `bootIntType` is not set
to "all", the function return a list of length equal to 1. 

```{r}
pr <- predict(fit, newdata=newd, bootInterval=TRUE, se.fit=TRUE,
              vcovMet="Jackknife", B.=100)
pr$norm
pr$stud
pr$bca
pr$perc
```

It is possible to speedup the process by changing the arguments
`parallel`, `ncpus` and `cl`. See the help file of the `boot` function
for more details. For example, the following would compute the
bootstrap interval in parallel using 8 CPU's

```{r, eval=FALSE}
pr <- predict(fit, newdata=newd, bootInterval=TRUE, se.fit=TRUE,
              vcovMet="Jackknife", B.=100, parallel="multicore",
              ncpus=8)
```

## Prediction intervals

The prediction intervals for LSLP and MALP are respectively:

\[
\left[\hat{Y}^\dagger(x_0) \pm z_{\alpha/2} \sqrt{S_\mathrm{Y}(1-\gamma^2)+
\sigma^2_\mathrm{LS}(x_0)/n}\right]
\]

and

\[
\left[\hat{Y}^\star(x_0) + \hat{b}(x_0) \pm z_{\alpha/2} \sqrt{S_\mathrm{Y}(1-\gamma^2)+
\sigma^2_\mathrm{MA}(x_0)/n}\right]
\]

where $\hat{b}(x_0)$ is the prediction bias of MALP. It turns out that
$\hat{Y}^\star(x_0) + \hat{b}(x_0)=\hat{Y}^\dagger(x_0)$, so the
prediction interval for MALP can be written as

\[
\left[\hat{Y}^\dagger(x_0) \pm z_{\alpha/2} \sqrt{S_\mathrm{Y}(1-\gamma^2)+
\sigma^2_\mathrm{MA}(x_0)/n}\right]
\]

Since $S_\mathrm{Y}(1-\gamma^2)$ is the estimated variance of the
error term of least squares models, if the argument `LSdfCorr` is set
to `TRUE`, $S_\mathrm{Y}(1-\gamma^2)$ is multiplied $(n-1)/df$, where
$df$ is the least squares residuals degrees of freedom. Note that
bootstrap intervals are not available for prediction intervals. 

```{r}
pr1 <- predict(fit, newdata=newd, interval="prediction",
               includeLS=TRUE)
pr2 <- predict(fit, newdata=newd, interval="confidence")
pr1$MALP
pr2
```

Since only the variance differs between the prediction interval of
LSLP amnd MALP, they are very close to each other:

```{r}
pr1$LSLP
```

## The `plot` method

This method produces a scatter plot of the dependent variable against
the fitted values from the MALP, LSLP or both. A 45 degree line is
added to better evaluate the level of agreement. The following shows
one example with `which="MALP"`. The other options are "LSLP" and
"Both".

```{r, fig.show="hold", out.width="50%"}
plot(fit, which="MALP", main="MALP predictor against Y", ylim=c(-6,8), col=c(1,1))
plot(fit, which="LSLP", main="LSLP predictor against Y", ylim=c(-6,8), col=c(1,1))
```

```{r, fig.align="center", out.width="50%"}
plot(fit, which="Both", main="MALP and LSLP predictors against Y", ylim=c(-6,8), col=c(1,1))
```
